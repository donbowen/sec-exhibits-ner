{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32e593a-acb6-447d-8388-0df65e554794",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python -m pip install textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b07bb745-4ca8-4184-a1ff-e3fb00559980",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import spacy\n",
    "from __future__ import unicode_literals\n",
    "import spacy,en_core_web_sm\n",
    "import textacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8043c7-1bfd-4c02-9bc0-e7f8f0e9a2a2",
   "metadata": {},
   "source": [
    "## Ner ner() function -- final version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c063a8c5-8c7d-4a48-92fc-4a8354d6583a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner(sentence): \n",
    "    \n",
    "    entities = []\n",
    "    verbs = []\n",
    "    subjects = []\n",
    "    \n",
    "    #Find the entities in the sentence\n",
    "    words  = nltk.word_tokenize(sentence)        # break down the sentence into words\n",
    "    tagged = nltk.pos_tag(words)                 # tag the words with Part of Speech \n",
    "    chunks = nltk.ne_chunk(tagged, binary=False) # binary = False named entities are classified (i.e PERSON, ORGANIZATION)\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        if hasattr(chunk, 'label'):              # hasattr(obj, key) -- checking if chunks have a label or not \n",
    "            entities.append(' '.join(c[0] for c in chunk)) # append entities to array\n",
    "    \n",
    "    #Find the verbs/subjects in the sentence\n",
    "    nlp = spacy.load(\"en_core_web_sm\")           # load in the spacy model\n",
    "    doc = nlp(sentence)                          # create spacy doc object\n",
    "    \n",
    "    verbs = [token.text for token in doc if token.pos_ == \"VERB\"]     # traverse thru the tokens, find the verbs\n",
    "    subjects = [token.text for token in doc if token.dep_ == \"nsubj\"]  # traverse thru the tokens, find the subjects\n",
    "        \n",
    "    return {'entities':entities, \n",
    "            'verbs':verbs,\n",
    "            'subjects':subjects}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d7089c6-3191-4c97-a717-1a253873a3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence = '\\nEX-10\\n2\\nex10-11.txt\\nCFC INTERNATIONAL, INC. - CONTRACT ADDENDUM\\n\\n ADDENDUM TO\\n PURCHASE AGREEMENT - DATED MARCH 1, 2001\\n\\nThis Agreement (the \"Addendum\"), effective October 15, 2002, between CFC\\nInternational, a Delaware corporation, (\"CFC\"), and Baxter Healthcare\\nCorporation, a Delaware corporation, and its successors, affiliates and assigns\\n(\"Baxter\"), amends the Purchase Agreement (\"Agreement\") between the two\\ncompanies dated March 1, 2001.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0361033e-f804-4d1e-88c8-f75aa25a39bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The cat sat on the mat and watched the bird.\"\n",
    "#sentence = 'Product Specifications may be revised from time to time and\\nnew Specifications and numbers added by mutual agreement between parties.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9bc211a-1d42-4397-9ca9-150be7d0cf18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entities': [], 'verbs': ['sat', 'watched'], 'subjects': ['cat']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = ner(sentence)\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596df466-d341-460a-9ff4-d7d6ea0d4ad9",
   "metadata": {},
   "source": [
    "## New ner() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62450584-c107-4c79-bbf0-78f64917cd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner(sentence): \n",
    "    \n",
    "    #Get the entities\n",
    "    words  = nltk.word_tokenize(sentence)        \n",
    "    tagged = nltk.pos_tag(words)                 \n",
    "    chunks = nltk.ne_chunk(tagged, binary=False) \n",
    "    \n",
    "    entities = []\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        if hasattr(chunk, 'label'): \n",
    "            entities.append(' '.join(c[0] for c in chunk)) \n",
    "    \n",
    "    #Get the verbs and subjects of the sentence\n",
    "    verbs = []\n",
    "    subjects = []\n",
    "    \n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(sentence)\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"VERB\":\n",
    "            verbs.append(token.text)\n",
    "        if token.dep_ == \"nsubj\":\n",
    "            subjects.append(token.text)\n",
    "    \n",
    "        \n",
    "    return {'entities':entities, \n",
    "            'verbs':verbs,\n",
    "            'subjects':subjects}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af44c9e-b12a-4e89-b53a-525eb0df4616",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The cat sat on the mat and watched the bird.\"\n",
    "temp = ner(sentence)\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f933f85b-af07-4f96-8ab9-49f6e9998dd4",
   "metadata": {},
   "source": [
    "## Prev ner() function [unchanged]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fa52dc-4c76-4a48-ac1c-5430fe252450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is some temp code: i'm trying to figure out how to find/save the verb\n",
    "# sentence = '13.7 In the event that a court of competent jurisdiction holds that\\nparticular provisions or requirements of this Agreement are in violation of any\\nlaw, such provisions or requirements shall be enforced and shall remain in full\\nforce and effect to the extent they are not in violation of any such law or not\\notherwise unenforceable, and all other provisions and requirements of this\\nAgreement shall remain in full force and effect.'\n",
    "# sentence = '13.6 This Agreement is deemed to have been entered into in the State of\\nIllinois and its interpretations, construction, and the remedies for its\\nenforcement of breach are to be applied pursuant to and in accordance with the\\nlaws of the State of Illinois.'\n",
    "#sentence = '\\nEX-10\\n2\\nex10-11.txt\\nCFC INTERNATIONAL, INC. - CONTRACT ADDENDUM\\n\\n ADDENDUM TO\\n PURCHASE AGREEMENT - DATED MARCH 1, 2001\\n\\nThis Agreement (the \"Addendum\"), effective October 15, 2002, between CFC\\nInternational, a Delaware corporation, (\"CFC\"), and Baxter Healthcare\\nCorporation, a Delaware corporation, and its successors, affiliates and assigns\\n(\"Baxter\"), amends the Purchase Agreement (\"Agreement\") between the two\\ncompanies dated March 1, 2001.'\n",
    "sentence = \"The cat sat on the mat and watched the bird.\"\n",
    "\n",
    "words = nltk.word_tokenize(sentence)         #break down the sentence into words\n",
    "tagged = nltk.pos_tag(words)                 #tag the words with Part of Speech \n",
    "chunks = nltk.ne_chunk(tagged, binary=False) #binary = False named entities are classified (i.e PERSON, ORGANIZATION)\n",
    "\n",
    "# todo experiement here to get verb and subject, once done, implement in function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e7e24c-d59b-441f-b314-5e863616d0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner(sentence): \n",
    "    \n",
    "    words  = nltk.word_tokenize(sentence)        # break down the sentence into words\n",
    "    tagged = nltk.pos_tag(words)                 # tag the words with Part of Speech \n",
    "    chunks = nltk.ne_chunk(tagged, binary=False) # binary = False named entities are classified (i.e PERSON, ORGANIZATION)\n",
    "    \n",
    "    entities = []\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        if hasattr(chunk, 'label'): # hasattr(obj, key) -- checking if chunks have a label or not \n",
    "            entities.append(' '.join(c[0] for c in chunk)) # append entities to array\n",
    "    \n",
    "    # todo add code here as needed to get the verb and subject, (if you get them via looping over chunks, then do within the for loop above)\n",
    "        \n",
    "    return {'entities':entities, \n",
    "           'random_out':np.random.uniform()  }  # todo update the output dictionary to output the verb and subject (and delete the placeholder \"random\" output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f90f79-eb9c-47fe-b649-28863cc0260d",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = ner(sentence)\n",
    "temp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
