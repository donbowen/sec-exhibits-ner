{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a71bc8-49a3-4bb2-b2ef-328344165188",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe85c0a-f930-4ce6-95fc-971780e48fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install stanza -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f880ec28-a8a4-4b45-ae9b-a79273045538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d17c9e752dc4472dab06b5faacca1cd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-29 18:16:01 INFO: Downloading default packages for language: en (English) ...\n",
      "2023-03-29 18:16:02 INFO: File exists: /Users/mikerich/stanza_resources/en/default.zip\n",
      "2023-03-29 18:16:06 INFO: Finished downloading models and saved to /Users/mikerich/stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "import torch;\n",
    "import stanza; stanza.download('en') # This downloads the English models for the neural pipelin\n",
    "#nlp = stanza.Pipeline('en', download_method=None) # This sets up a default neural pipeline in English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c65ea3a3-7fa2-4354-9360-e509c612a983",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "from __future__ import unicode_literals\n",
    "import spacy,en_core_web_sm\n",
    "import textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb001718-68c7-40e7-96bf-24f6ea1b67ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/mikerich/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/mikerich/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/mikerich/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/mikerich/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package state_union to\n",
      "[nltk_data]     /Users/mikerich/nltk_data...\n",
      "[nltk_data]   Package state_union is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('words')\n",
    "nltk.download('punkt')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('state_union')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ce0b5c-d622-4f76-b30d-004cf5dd0f82",
   "metadata": {},
   "source": [
    "### One sentence test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5470702f-babd-4408-81aa-65fac786b0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Seller produces hot stamping foil which conforms and meets the Specification Requirements submitted, accepted and in Seller's possession for the Specification numbers listed attached in the Exhibit A., hereafter referred nto as 'Products'.\"\n",
    "\n",
    "doc = nlp(sentence)\n",
    "\n",
    "subj_verb_linkages = []\n",
    "\n",
    "for word in doc.sentences[0].words:    # for each word in the sentence\n",
    "    if word.deprel == 'root':          # if a word is the root, its dependency relation label is 'root'. thus if this is true, the curr word = root word\n",
    "        \n",
    "        root_verb = word.text          # save the root verb  \n",
    "        root_id = word.id              # get the words id \n",
    "        \n",
    "        for w in doc.sentences[0].words:                      # loop over words in the sentence\n",
    "            if w.head == root_id and w.deprel == 'nsubj':     # if words head attribute = root_id, then its a a direct dependent of the root (is a child of the root)\n",
    "                \n",
    "                subject = w.text                              # save the associated subject\n",
    "        \n",
    "\n",
    "subj_verb_linkages = [subject, root_verb]\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Root Verb: {root_verb}, Subject: {subject}\")\n",
    "\n",
    "subj_verb_linkages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5985fce8-fceb-4ddf-89f1-202452b451db",
   "metadata": {},
   "source": [
    "### New ner() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19b6ea97-0878-4b75-a51a-3d209b748a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner(sentence): \n",
    "    \n",
    "    entities = []\n",
    "    verbs = []\n",
    "    subjects = []\n",
    "    subj_verb_linkages = []\n",
    "    \n",
    "    #Find the entities in the sentence\n",
    "    words  = nltk.word_tokenize(sentence)        # break down the sentence into words\n",
    "    tagged = nltk.pos_tag(words)                 # tag the words with Part of Speech \n",
    "    chunks = nltk.ne_chunk(tagged, binary=False) # binary = False named entities are classified (i.e PERSON, ORGANIZATION)\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        if hasattr(chunk, 'label'):              # hasattr(obj, key) -- checking if chunks have a label or not \n",
    "            entities.append(' '.join(c[0] for c in chunk)) # append entities to array\n",
    "    \n",
    "    \n",
    "    #Find the verbs/subjects in the sentence\n",
    "    nlp = spacy.load(\"en_core_web_sm\")           # load in the spacy model\n",
    "    doc = nlp(sentence)                          # create spacy doc object\n",
    "    \n",
    "    verbs = [token.text for token in doc if token.pos_ == \"VERB\"]     # traverse thru the tokens, find the verbs\n",
    "    subjects = [token.text for token in doc if token.dep_ == \"nsubj\"]  # traverse thru the tokens, find the subjects\n",
    "    \n",
    "    \n",
    "    #Find the Root Subject-verb linkages in the sentences using stanza\n",
    "    nlp = stanza.Pipeline('en', download_method=None)            # this sets up a default neural pipeline in English\n",
    "    doc = nlp(sentence)\n",
    "     \n",
    "    for word in doc.sentences[0].words:    # for each word in the sentence\n",
    "        if word.deprel == 'root':          # if a word is the root, its dependency relation label is 'root'. thus if this is true, the curr word = root word\n",
    "\n",
    "            root_verb = word.text          # save the root verb  \n",
    "            root_id = word.id              # get the words id \n",
    "            \n",
    "            for w in doc.sentences[0].words:                      # loop over words in the sentence\n",
    "                if w.head == root_id and w.deprel == 'nsubj':     # if words head attribute = root_id, then its a a direct dependent of the root (is a child of the root)\n",
    "                    subject = w.text \n",
    "                \n",
    "    subj_verb_linkages = [subject, root_verb]   # subj_verb linkages array \n",
    "    \n",
    "        \n",
    "    return {'entities':entities, \n",
    "            'verbs':verbs,\n",
    "            'subjects':subjects,\n",
    "            'subj_verb_linkages':subj_verb_linkages} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e7e38e3-e36b-470d-ae61-08dfae9a7d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-29 18:16:09 INFO: Loading these models for language: en (English):\n",
      "============================\n",
      "| Processor    | Package   |\n",
      "----------------------------\n",
      "| tokenize     | combined  |\n",
      "| pos          | combined  |\n",
      "| lemma        | combined  |\n",
      "| constituency | wsj       |\n",
      "| depparse     | combined  |\n",
      "| sentiment    | sstplus   |\n",
      "| ner          | ontonotes |\n",
      "============================\n",
      "\n",
      "2023-03-29 18:16:09 INFO: Using device: cpu\n",
      "2023-03-29 18:16:09 INFO: Loading: tokenize\n",
      "2023-03-29 18:16:09 INFO: Loading: pos\n",
      "2023-03-29 18:16:09 INFO: Loading: lemma\n",
      "2023-03-29 18:16:09 INFO: Loading: constituency\n",
      "2023-03-29 18:16:09 INFO: Loading: depparse\n",
      "2023-03-29 18:16:10 INFO: Loading: sentiment\n",
      "2023-03-29 18:16:10 INFO: Loading: ner\n",
      "2023-03-29 18:16:11 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Seller produces hot stamping foil which conforms and meets the Specification Requirements submitted, accepted and in Seller's possession for the Specification numbers listed attached in the Exhibit A., hereafter referred nto as 'Products'.\"\n",
    "\n",
    "temp = ner(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0593aedd-f0eb-47e7-9912-a87c12dea282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entities': ['Seller', 'Requirements', 'Seller', 'Exhibit'],\n",
       " 'verbs': ['produces',\n",
       "  'stamping',\n",
       "  'conforms',\n",
       "  'meets',\n",
       "  'submitted',\n",
       "  'accepted',\n",
       "  'listed',\n",
       "  'attached',\n",
       "  'referred'],\n",
       " 'subjects': ['Seller', 'which', 'hereafter'],\n",
       " 'subj_verb_linkages': ['Seller', 'produces']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
